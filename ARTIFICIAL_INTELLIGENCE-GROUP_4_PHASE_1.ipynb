{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM DEFINITION AND DESIGN THINKING DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM STATEMENT\n",
    "\n",
    "The problem at hand involves performing sentiment analysis on customer\n",
    "feedback to gain valuable insights into competitor products. By understanding customer sentiments,\n",
    "companies can identify strengths and weaknesses in competing products, thereby improving their\n",
    "own offerings. This project requires leveraging various Natural Language Processing (NLP) methods to\n",
    "extract meaningful insights from customer feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DESIGN THINKING APPROACH\n",
    "\n",
    "To effectively solve the problem of performing sentiment analysis on customer feedback, we will follow a structured design thinking approach. This approach involves several key steps, as outlined below:\n",
    "\n",
    "##### DATA COLLECTION\n",
    "\n",
    "OBJECTIVE: Identify a dataset containing customer reviews and sentiments about competitor products.\n",
    "\n",
    "DATA SOURCES: Identify and gather relevant datasets from sources such as online review platforms, social media, or customer feedback forms.\n",
    "\n",
    "DATA VOLUME: Ensure the dataset is sufficiently large to provide meaningful insights and a representative sample of customer sentiments.\n",
    "\n",
    "DATA QUALITY: Verify the data quality by checking for missing values,duplicates, and inconsistencies.\n",
    "\n",
    "##### DATA PREPROCESSING\n",
    "\n",
    "OBJECTIVE: Clean and preprocess the textual data for analysis.\n",
    "\n",
    "TEXT CLEANING: Remove any special characters, punctuation, and irrelevant information that may not contribute to sentiment analysis.\n",
    "\n",
    "TOKENIZATION: Split the text into individual words or tokens for further analysis.\n",
    "\n",
    "STOPWORD REMOVAL: Eliminate common stopwords (e.g., “and,” “the,” “is”) to reduce noise in the data.\n",
    "\n",
    "NORMALZATION: Normalize text by converting it to lowercase to ensure consistent.\n",
    "\n",
    "##### SENTIMENT ANALYSIS TECHNIQUES\n",
    "\n",
    "OBJECTIVE:Employ different NLP techniques like Bag of Words, Word Embeddings, or Transformer models for sentiment analysis.\n",
    "\n",
    "BAG OF WORDS (BOW): Create a BoW representation of the text data, which counts the frequency of words in each document.\n",
    "\n",
    "WORD EMBEDDINGS: Utilize pre-trained word embeddings to capture semantic meaning and relationships between words.\n",
    "\n",
    "TRANSFORMER MODELS :Leverage advanced transformer-based models for deep contextualized sentiment analysis.\n",
    "\n",
    "##### FEATURE EXTRACTION\n",
    "\n",
    "OBJECTIVE : Extract features and sentiments from the text data.\n",
    "\n",
    "SENTIMENT SCORING: Assign sentiment scores (positive, negative, neutral) to each customer review using the chosen sentiment analysis technique.\n",
    "\n",
    "FEATURE EXTRACTION: Extract relevant features from the text, such as product mentions, key phrases, or specific attributes that customers mention in their feedback.\n",
    "\n",
    "##### VISUALIZATION  \n",
    "\n",
    "OBJECTIVE : Create visualizations to depict the sentiment distribution and analyze trends.\n",
    "\n",
    "SENTIMENT DISTRIBUTION : Visualize the distribution of sentiment scores using bar charts, histograms, or pie charts to understand the overall sentiment of customer feedback  \n",
    "\n",
    "TEMPORARAL ANALYSIS: Track sentiment trends over time to identify patterns or changes in customer perceptions.\n",
    "\n",
    "WORD CLOUDS: Generate word clouds to highlight frequently mentioned words or phrases in customer reviews.\n",
    "\n",
    "##### INSIGHTS GENERATION\n",
    "\n",
    "OBJECTIVE: Extract meaningful insights from the sentiment analysis results to guide business decisions.\n",
    "\n",
    "COMPETITOR ANALYSIS: Compare the sentiment scores of competitor products to identify strengths and weaknesses.\n",
    "\n",
    "CUSTOMER FEEDBACK TRENDS: Identify recurring themes or issues in customer feedback that require attention.\n",
    "\n",
    "RECOMMENDATIONS : Provide actionable recommendations for product improvement or marketing strategies based on the insights gained.\n",
    "\n",
    "This dataset consist of the reviews submitted by the individuals who traveled through various Airlines.\n",
    "\n",
    "Here in the dataset the reviews are observed and based on the observation it is categorized in to 3 categories: Positive, Negative, Neutral.\n",
    "\n",
    "PURPOSE: The purpose is to create a model which on providing the tweets (reviews) to the training should provide the the outcome that whether a particular tweet done by a individual is a positive response or a negative one or neutral.\n",
    "\n",
    "All reviews that depict positive response contains good experience of the traveler with airlines.\n",
    "\n",
    "All reviews that depict negative response contains difficulty faced by traveler.\n",
    "\n",
    "Neutral responses will be, that which are not specific to be considered in positive or negative.\n",
    "\n",
    "OUTCOME : This will help the Airline find out their deficiencies so that they could work on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(os.listdir('../input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cells define functions for plotting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check 1st file: ../input/Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
    "# Tweets.csv has 14640 rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "df1 = pd.read_csv('../input/Tweets.csv', delimiter=',', nrows = nRowsRead)\n",
    "df1.dataframeName = 'Tweets.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution graphs (histogram/bar graph) of sampled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(df1, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This design thinking approach outlines a structured methodology for tackling the problem of performing sentiment analysis on customer feedback to gain insights into competitor products. By following these steps, we aim to extract valuable information from textual data,visualize trends, and generate actionable insights that can drive informed business decisions and enhance the company’s competitive edge in the market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
